
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: Ungraded Practice Notebook\n",
    "\n",
    "In this ungraded notebook, you'll try out all the individual techniques that you learned about in the lecture. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\n",
    "\n",
    "This notebook is made of two main parts: data preparation, and the continuous bag-of-words (CBOW) model.\n",
    "\n",
    "To get started, import and initialize all the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: emoji\n",
      "Successfully installed emoji-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import numpy as np\n",
    "\n",
    "from utils2 import get_dict\n",
    "\n",
    "nltk.download('punkt')  # download pre-trained Punkt tokenizer for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data preparation phase, starting with a corpus of text, you will:\n",
    "\n",
    "- Clean and tokenize the corpus.\n",
    "\n",
    "- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n",
    "\n",
    "- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and tokenization\n",
    "\n",
    "To demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
      "After cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus:  {corpus}')\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "print(f'After cleaning punctuation:  {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use NLTK's tokenization engine to split the corpus into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial string:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
      "After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial string:  {data}')\n",
    "data = nltk.word_tokenize(data)\n",
    "print(f'After tokenization:  {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n",
      "After cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial list of tokens:  {data}')\n",
    "data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()\n",
    "         or ch == '.'\n",
    "         or emoji.get_emoji_regexp().search(ch)\n",
    "       ]\n",
    "print(f'After cleaning:  {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the heart emoji is considered as a token just like any normal word.\n",
    "\n",
    "Now let's streamline the cleaning and tokenization process by wrapping the previous steps in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.'\n",
    "             or emoji.get_emoji_regexp().search(ch)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this function to the corpus that you'll be working on in the rest of this notebook: \"I am happy because I am learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'I am happy because I am learning'\n",
    "print(f'Corpus:  {corpus}')\n",
    "words = tokenize(corpus)\n",
    "print(f'Words (tokens):  {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now try it out yourself with your own sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Now it's your turn: try with your own sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\n",
    "\n",
    "The `get_windows` function in the next cell was introduced in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. Recall that for a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word.\n",
    "\n",
    "Here is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(\n",
    "            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n",
    "            2\n",
    "        ):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example of the training set is made of:\n",
    "\n",
    "- the context words \"i\", \"am\", \"because\", \"i\",\n",
    "\n",
    "- and the center word to be predicted: \"happy\".\n",
    "\n",
    "**Now try it out yourself. In the next cell, you can change both the sentence and the context half-size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'your']\tit\n",
      "['it', 'turn']\tyour\n",
      "['your', 'try']\tturn\n",
      "['turn', 'with']\ttry\n",
      "['try', 'your']\twith\n",
      "['with', 'own']\tyour\n",
      "['your', 'sentence']\town\n",
      "['own', '.']\tsentence\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming words into vectors for the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish preparing the training set, you need to transform the context words and center words into vectors.\n",
    "\n",
    "### Mapping words to indices and indices to words\n",
    "\n",
    "The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n",
    "\n",
    "To create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, `get_dict`, that creates a Python dictionary that maps words to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the dictionary that maps words to numeric indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this dictionary to get the index of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'i':   3\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of the word 'i':  \",word2Ind['i'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And conversely, here's the dictionary that maps indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word which has index 2:   happy\n"
     ]
    }
   ],
   "source": [
    "print(\"Word which has index 2:  \",Ind2word[2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting one-hot word vectors\n",
    "\n",
    "Recall from the lecture that you can easily convert an integer, $n$, into a one-hot vector.\n",
    "\n",
    "Consider the word \"happy\". First, retrieve its numeric index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = word2Ind['happy']\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a vector with the size of the vocabulary, and fill it with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_word_vector = np.zeros(V)\n",
    "center_word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that the vector has the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(center_word_vector) == V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, replace the 0 of the $n$-th element with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_word_vector[n] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you have your one-hot word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    # BEGIN your code here\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    # END your code here\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('happy', word2Ind, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the word vector for \"learning\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "word_to_one_hot_vector('learning', word2Ind, V)\n",
    "# END your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([0., 0., 0., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting context word vectors"