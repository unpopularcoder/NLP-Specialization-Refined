{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wmf07eJE8p9r"
   },
   "source": [
    "# Assignment 4: Chatbot\n",
    "\n",
    "<img src = \"cbot.jpg\" height=\"400\" width=\"400\"> \n",
    "\n",
    "Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the [Reformer](https://arxiv.org/abs/2001.04451), also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:\n",
    "\n",
    "* Understand how the Reformer works\n",
    "* Explore the [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
    "* Process the data to feed it into the model\n",
    "* Train your model\n",
    "* Generate a dialogue by feeding a question to the model\n",
    "\n",
    "\n",
    "## Outline\n",
    "- [Part 1:   Exploring the MultiWoz dataset](#1)\n",
    "\t- [Exercise 01](#ex01)\n",
    "- [Part 2:   Processing the data for Reformer inputs](#2)\n",
    "    - [2.1   Tokenizing, batching with bucketing](#2.1)\n",
    "- [Part 3:   Reversible layers](#3)\n",
    "\t- [Exercise 02](#ex02)\n",
    "\t- [Exercise 03](#ex03)\n",
    "    - [3.1   Reversible layers and randomness](#3.1)\n",
    "- [Part 4:   ReformerLM Training](#4)\n",
    "\t- [Exercise 04](#ex04)\n",
    "\t- [Exercise 05](#ex05)\n",
    "- [Part 5:   Decode from a pretrained model](#5)\n",
    "\t- [Exercise 06](#ex06)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1:   Exploring the MultiWoz dataset\n",
    "\n",
    "You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the modules we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "aV4zpTnSVFIp",
    "outputId": "e3a85dd1-e375-4636-ea62-b9b403f0952a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                     1.3.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "import trax   \n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also declare some constants we will be using in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename of the MultiWOZ dialogue dataset\n",
    "DATA_FILE = 'data.json'\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# dictionary where we will load the dialogue dataset\n",
    "DIALOGUE_DB = {}\n",
    "\n",
    "# vocabulary filename\n",
    "VOCAB_FILE = 'en_32k.subword'\n",
    "\n",
    "# vocabulary file directory\n",
    "VOCAB_DIR = 'data/vocabs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "K58I5vFB7GlP",
    "outputId": "3d086ea4-7898-4870-b52f-f362cb02e118"
   },
   "outputs": [],
   "source": [
    "# help function to load a JSON file\n",
    "def load_json(directory, file):\n",
    "    with open(f'{directory}/{file}') as file: \n",
    "        db = json.load(file)\n",
    "    return db\n",
    "\n",
    "# load the dialogue data set into our dictionary\n",
    "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "VGBnUfEk8p9x",
    "outputId": "4b364506-1088-4f00-be0c-4fefa892dc4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dialogues is: 10438\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dialogues are composed of multiple files and the filenames are used as keys in 