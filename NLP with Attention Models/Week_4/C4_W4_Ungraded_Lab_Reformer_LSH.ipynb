{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVHY_ZYxWDph"
   },
   "source": [
    "# Reformer Efficient Attention: Ungraded Lab\n",
    "The videos describe two 'reforms' made to the Transformer to make it more memory and compute efficient. The *Reversible Layers* reduce memory and *Locality Sensitive Hashing(LSH)* reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\n",
    "\n",
    "Specifically, the notebook has 3 goals\n",
    "* review dot-product self attention for reference\n",
    "* examine LSH based self attention\n",
    "* extend our understanding and familiarity with Trax infrastructure\n",
    "\n",
    "## Outline\n",
    "- [Part 1:  Trax Efficient Attention classes](#1)\n",
    "- [Part 2:  Full Dot Product Self Attention](#2)\n",
    "    - [2.1  Description](#2.1)\n",
    "        - [2.1.1  our_softmax](#2.1.1)\n",
    "    - [2.2  our simple attend](#2.2)\n",
    "    - [2.3  Class OurSelfAttention](#2.3)\n",
    "- [Part 3:  Trax LSHSelfAttention](#3)\n",
    "    - [3.1  Description](#3.1)\n",
    "    - [3.2  our_hash_vectors](#3.2)\n",
    "    - [3.3  Sorting Buckets](#3.3)\n",
    "    - [3.4  Chunked dot product attention](#3.4)\n",
    "    - [3.5  OurLSHSelfAttention](#3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nhevIw88Vm2"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## Part 1.0  Trax Efficient Attention classes\n",
    "Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses 'layers' as a useful level of abstraction. Layers are often represented as *classes*. We're going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the 'forward' functions and utilize the existing attention layers as parent classes. The original code can be found at [github:trax/layers/Research/Efficient_attention](https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py). This link references release 1.3.4 but note that this is under the 'research' directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:\n",
    "<img src = \"C4W4_LN2_image11.PNG\" height=\"250\" width=\"250\">\n",
    "<center><b>Figure 1: Reference Tag 1.3.4 on github</b></center>\n",
    "\n",
    "\n",
    "\n",
    "While Trax uses classes liberally, we have not built many classes in the course so far. Let's spend a few moments reviewing the classes we will be using.\n",
    "<img src = \"C4W4_LN2_image1.PNG\" height=\"788\" width=\"1561\">\n",
    "\n",
    "<center><b>Figure 2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing.</b></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the *Forward* routine. It supports a `use_reference_code` capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each *'example, head'* independently. This simplifies our work as we need only worry about matrix operations on one *'example, head'* at a time. This loop calls *forward_unbatched*, which is the child process that we will be overriding.\n",
    "\n",
    "On the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a 'traditional' implementation of the dot product attention. We will be implementing the *forward_unbatched* version of this to highlight the differences between this and the LSH implementation.\n",
    "\n",
    "Below that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the *forward_unbatched* section of this and some of the utility functions it uses to explore its implementation in more detail.\n",
    "\n",
    "The code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The [Trax documentation](https://trax-ml.readthedocs.io/en/latest/) can also be referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.2\"></a>\n",
    "## Part 1.2  Trax Details\n",
    "The goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:\n",
    "* Trax operates with multiple back-end libraries, we will see special cases that will utilize unique features.\n",
    "* 'Fancy' numpy indexing is not supported in all backend environments and must be emulated in other ways.\n",
    "* Some operations don't have gradients for backprop and must be ignored or include forced re-evaluation.\n",
    "\n",
    "Here are some of the functions we may see:\n",
    "* Abstracted as `fastmath`, Trax supports multiple backend's such as [Jax](https://github.com/google/jax) and [Tensorflow2](https://github.com/tensorflow/tensorflow)\n",
    "* [tie_in](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.tie_in.html): Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are 'tied' to other numeric operations using tie_in.\n",
    "* [stop_gradient](https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html): Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.\n",
    "* Below we will execute `from trax.fastmath import numpy as np `, this uses accelerated forms of numpy functions. This is, however a *subset* of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGLyG_n_4WhH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "from trax import layers as tl  # core building block\n",
    "import jax\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids\n",
    "\n",
    "# fastmath.use_backend('tensorflow-numpy')\n",
    "import functools\n",
    "from trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\n",
    "from trax.layers import (\n",
    "    tie_in,\n",
    "    length_normalized,\n",
    "    apply_broadcasted_dropout,\n",
    "    look_adjacent,\n",
    "    permute_via_gather,\n",
    "    permute_via_sort,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Part 2  Full Dot-Product Self Attention\n",
    "<a name=\"1.2\"></a>\n",
    "### Part 2.1 Description\n",
    "<img src = \"C4W4_LN2_image2.PNG\" height=\"200\" width=\"600\">\n",
    "\n",
    "<center><b>Figure 3: Project datapath and primary data structures and where they are implemented</b></center>\n",
    "\n",
    "The diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on *our_simple_attend* or our simpler version of the original *attend* function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.\n",
    "\n",
    "<img src = \"C4W4_LN2_image3.PNG\" height=\"250\" width=\"700\">\n",
    "\n",
    "<center><b>Figure 4: dot-product of Query and Key</b></center>\n",
    "\n",
    "The *attend* function receives *Query* and *Key*. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as *embeddings* assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices $W^Q$ and $W^K$. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in *attend* is matmul(q,q). Note the resulting dot-product (*Dot*) entries describe a complete (n_seq,n_seq) map of the similarity of all en