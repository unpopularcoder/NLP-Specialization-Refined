{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVHY_ZYxWDph"
   },
   "source": [
    "# Reformer Efficient Attention: Ungraded Lab\n",
    "The videos describe two 'reforms' made to the Transformer to make it more memory and compute efficient. The *Reversible Layers* reduce memory and *Locality Sensitive Hashing(LSH)* reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\n",
    "\n",
    "Specifically, the notebook has 3 goals\n",
    "* review dot-product self attention for reference\n",
    "* examine LSH based self attention\n",
    "* extend our understanding and familiarity with Trax infrastructure\n",
    "\n",
    "## Outline\n",
    "- [Part 1:  Trax Efficient Attention classes](#1)\n",
    "- [Part 2:  Full Dot Product Self Attention](#2)\n",
    "    - [2.1  Description](#2.1)\n",
    "        - [2.1.1  our_softmax](#2.1.1)\n",
    "    - [2.2  our simple attend](#2.2)\n",
    "    - [2.3  Class OurSelfAttention](#2.3)\n",
    "- [Part 3:  Trax LSHSelfAttention](#3)\n",
    "    - [3.1  Description](#3.1)\n",
    "    - [3.2  our_hash_vectors](#3.2)\n",
    "    - [3.3  Sorting Buckets](#3.3)\n",
    "    - [3.4  Chunked dot product attention](#3.4)\n",
    "    - [3.5  OurLSHSelfAttention](#3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nhevIw88Vm2"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## Part 1.0  Trax Efficient Attention classes\n",
    "Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses 'layers' as a useful level of abstraction. Layers are often represented as *classes*. We're going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the 'forward' functions and utilize the existing attention lay