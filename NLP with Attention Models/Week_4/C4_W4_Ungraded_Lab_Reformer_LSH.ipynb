{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVHY_ZYxWDph"
   },
   "source": [
    "# Reformer Efficient Attention: Ungraded Lab\n",
    "The videos describe two 'reforms' made to the Transformer to make it more memory and compute efficient. The *Reversible Layers* reduce memory and *Locality Sensitive Hashing(LSH)* reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\n",
    "\n",
    "Specifically, the notebook has 3 goals\n",
    "* review dot-product self attention for reference\n",
    "* examine LSH based self attention\n",
    "* extend our understanding and familiarity with Trax infrastructure\n",
    "\n",
    "## Outline\n",
    "- [Part 1:  Trax Efficient Attention classes](#1)\n",
    "- [Part 2:  Full Dot Product Self Attention](#2)\n",
    "    - [2.1  Description](#2.1)\n",
    "        - [2.1.1  our_softmax](#2.1.1)\n",
    "    - [2.2  our simple attend](#2.2)\n",
    "    - [2.3  Class OurSelfAttention](#2.3)\n",
    "- [Part 3:  Trax LSHSelfAttention](#3)\n",
    "    - [3.1  Description](#3.1)\n",
    "    - [3.2  our_hash_vectors](#3.2)\n",
    "    - [3.3  Sorting Buckets](#3.3)\n",
    "    - [3.4  Chunked dot product attention](#3.4)\n",
    "    - [3.5  OurLSHSelfAttention](#3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nhevIw88Vm2"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## Part 1.0  Trax Efficient Attention classes\n",
    "Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses 'layers' as a useful level of abstraction. Layers are often represented as *classes*. We're going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the 'forward' functions and utilize the existing attention layers as parent classes. The original code can be found at [github:trax/layers/Research/Efficient_attention](https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py). This link references release 1.3.4 but note that this is under the 'research' directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:\n",
    "<img src = \"C4W4_LN2_image11.PNG\" height=\"250\" width=\"250\">\n",
    "<center><b>Figure 1: Reference Tag 1.3.4 on github</b></center>\n",
    "\n",
    "\n",
    "\n",
    "While Trax uses classes liberally, we have not built many classes in the course so far. Let's spend a few moments reviewing the classes we will be using.\n",
    "<img src = \"C4W4_LN2_image1.PNG\" height=\"788\" width=\"1561\">\n",
    "\n",
    "<center><b>Figure 2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing.</b></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the *Forward* routine. It supports a `use_reference_code` capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each *'example, head'* independently. This simplifies our work as we need only worry about matrix operations on one *'example, head'* at a time. This loop calls *forward_unbatched*, which is the child process that we will be overriding.\n",
    "\n",
    "On the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a 'traditional' implementation of the dot product attention. We will be implementing the *forward_unbatched* version of this to highlight the differences between this and the LSH implementation.\n",
    "\n",
    "Below that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the *forward_unbatched* section of this and some of the utility functions it uses to explore its implementation in more detail.\n",
    "\n",
    "The code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The [Trax documentation](https://trax-ml.readthedocs.io/en/latest/) can also be referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.2\"></a>\n",
    "## Part 1.2  Trax Details\n",
    "The goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:\n",
    "* Trax operates with multiple back-end libraries, we will see special cases that will utilize unique features.\n",
    "* 'Fancy' numpy indexing is not supported in all backend environments and must be emulated in other ways.\n",
    "* Some operations don't have gradients for backprop and must be ignored or include forced re-evaluation.\n",
    "\n",
    "Here are some of the functions we may see:\n",
    "* Abstracted as `fastmath`, Trax supports multiple backend's such as [Jax](https://github.com/google/jax) and [Tensorflow2](https://github.com/tensorflow/tensorflow)\n",
    "* [tie_in](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.tie_in.html): Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are 'tied' to other numeric operations using tie_in.\n",
    "* [stop_gradient](https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html): Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.\n",
    "* Below we will execute `from trax.fastmath import numpy as np `, this uses accelerated forms of numpy functions. This is, however a *subset* of numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGLyG_n_4WhH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "from trax import layers as tl  # core building block\n",
    "import jax\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids\n",
    "\n",
    "# fastmath.use_backend('tensorflow-numpy')\n",
    "import functools\n",
    "from trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\n",
    "from trax.layers import (\n",
    "    tie_in,\n",
    "    length_normalized,\n",
    "    apply_broadcasted_dropout,\n",
    "    look_adjacent,\n",
    "    permute_via_gather,\n",
    "    permute_via_sort,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Part 2  Full Dot-Product Self Attention\n",
    "<a name=\"1.2\"></a>\n",
    "### Part 2.1 Description\n",
    "<img src = \"C4W4_LN2_image2.PNG\" height=\"200\" width=\"600\">\n",
    "\n",
    "<center><b>Figure 3: Project datapath and primary data structures and where they are implemented</b></center>\n",
    "\n",
    "The diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on *our_simple_attend* or our simpler version of the original *attend* function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.\n",
    "\n",
    "<img src = \"C4W4_LN2_image3.PNG\" height=\"250\" width=\"700\">\n",
    "\n",
    "<center><b>Figure 4: dot-product of Query and Key</b></center>\n",
    "\n",
    "The *attend* function receives *Query* and *Key*. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as *embeddings* assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices $W^Q$ and $W^K$. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in *attend* is matmul(q,q). Note the resulting dot-product (*Dot*) entries describe a complete (n_seq,n_seq) map of the similarity of all entries of q vs all entries of k. This is reflected in the notation in the dot-product boxes of $w_n$,$w_m$ representing word_n, word_m. Note that each row of *Dot* describes the relationship of an input embedding, say $w_0$, with every other input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some applications some values are masked. This can be used, for example to exclude results that occur later in time (causal) or to mask padding or other inputs.\n",
    "<img src = \"C4W4_LN2_image4.PNG\" height=\"300\" width=\"900\">\n",
    "\n",
    "<center><b>Figure 5: Masking</b></center>\n",
    "\n",
    "\n",
    "The routine below *mask_self_attention* implements a flexible masking capability. The masking is controlled by the information in q_info and kv_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDAWq_xMX6C1"
   },
   "outputs": [],
   "source": [
    "def mask_self_attention(\n",
    "    dots, q_info, kv_info, causal=True, exclude_self=True, masked=False\n",
    "):\n",
    "    \"\"\"Performs masking for self-attention.\"\"\"\n",
    "    if causal:\n",
    "        mask = fastmath.lt(q_info, kv_info).astype(np.float32)\n",
    "        dots = dots - 1e9 * mask\n",
    "    if exclude_self:\n",
    "        mask = np.equal(q_info, kv_info).astype(np.float32)\n",
    "        dots = dots - 1e5 * mask\n",
    "    if masked:\n",
    "        zeros_like_kv_info = tie_in(kv_info, np.zeros_like(kv_info))\n",
    "        mask = fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)\n",
    "        dots = dots - 1e9 * mask\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SoftMax is applied per row of the *Dot* matrix to scale the values in the row between 0 and 1.\n",
    "<img src = \"C4W4_LN2_image5.PNG\" height=\"300\" width=\"900\">\n",
    "\n",
    "<center><b>Figure 6: SoftMax per row of Dot</b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1.1\"></a>\n",
    "### Part 2.1.1 our_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses a separable form of the softmax calculation. Recall the softmax:\n",
    "$$ softmax(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\tag{1}$$\n",
    "This can be alternately implemented as:\n",
    "$$ logsumexp(x)=\\log{({\\sum_j \\exp(x_j)})}\\tag{2}$$\n",
    "$$ softmax(x_i)=\\exp({x_i - logsumexp(x)})\\tag{3}$$\n",
    "The work below will maintain a copy of the logsumexp allowing the softmax to be completed in sections. You will see how this is useful later in the LSHSelfAttention class.\n",
    "We'll create a routine to implement that here with the addition of a passthrough. The matrix operations we will be working on below are easier to follow if we can maintain integer values. So, for tests, we will skip the softmax in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_softmax(x, passthrough=False):\n",
    "    \"\"\" softmax with passthrough\"\"\"\n",
    "    logsumexp = fastmath.logsumexp(x, axis=-1, keepdims=True)\n",
    "    o = np.exp(x - logsumexp)\n",
    "    if passthrough:\n",
    "        return (x, np.zeros_like(logsumexp))\n",
    "    else:\n",
    "        return (o, logsumexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## compare softmax(a) using both methods\n",
    "a = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "sma = np.exp(a) / sum(np.exp(a))\n",
    "print(sma)\n",
    "sma2, a_logsumexp = our_softmax(a)\n",
    "print(sma2)\n",
    "print(a_logsumexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the dot-product is to 'focus attention' on some of the inputs. Dot now has entries appropriately scaled to enhance some values and reduce others. These are now applied to the $V$ entries.\n",
    "<img src = \"C4W4_LN2_image6.PNG\" height=\"300\" width=\"900\">\n",
    "\n",
    "<center><b>Figure 7: Applying Attention to $V$</b></center>\n",
    "\n",
    "$V$ is of size (n_seq,n_v). Note the shading in the diagram. This is to draw attention to the operation of the matrix multiplication. This is detailed below.\n",
    "\n",
    "<img src = \"C4W4_LN2_image7.PNG\" height=\"300\" width=\"600\"/>\n",
    "\n",
    "<center><b>Figure 7: The Matrix Multiply applies attention to the values of V</b></center>\n",
    "\n",
    "$V$ is formed by a matrix multiply of the input embedding with the weight matrix $W^v$ whose values were set by backpropagation. The row entries of $V$ are then related to the corresponding input embedding. The matrix multiply weights first column of V, representing a section of each of the input embeddings, with the first row of Dot, representing the similarity of $W_0$ and each word of the input embedding and deposits the value in $Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### Part 2.2  our_simple_attend\n",
    "In this section we'll work on an implementation of *attend* whose operations you can see in figure 3. It is a slightly simplified version of the routine in [efficient_attention.py](https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py). We will fill in a few lines of code. The main goal is to become familiar with the routine. You have implemented similar functionality in a previous assignment.\n",
    "\n",
    "**Instructions**\n",
    "**Step 1:** matrix multiply (np.matmul) q and the k 'transpose' kr.\n",
    "**Step 2:** use our_softmax() to perform a softmax on masked output of the dot product, dots.\n",
    "**Step 3:** matrix multiply (np.matmul) dots and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGvWTWQ4ExAx"
   },
   "outputs": [],
   "source": [
    "def our_simple_attend(\n",
    "    q,\n",
    "    k=None,\n",
    "    v=None,\n",
    "    mask_fn=None,\n",
    "    q_info=None,\n",
    "    kv_info=None,\n",
    "    dropout=0.0,\n",
    "    rng=None,\n",
    "    verbose=False,\n",
    "    passthrough=False,\n",
    "):\n",
    "    \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n",
    "\n",
    "  Args:\n",
    "    q: Query vectors, shape [q_len, d_qk]\n",
    "    k: Key vectors, shape [kv_len, d_qk]; or None\n",
    "    v: Value vectors, shape [kv_len, d_v]\n",
    "    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n",
    "    q_info: Query-associated metadata for masking\n",
    "    kv_info: Key-associated metadata for masking\n",
    "    dropout: Dropout rate\n",
    "    rng: RNG for dropout\n",
    "\n",
    "  Returns:\n",
    "    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n",
    "    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n",
    "    probabilities is useful for combining multiple rounds of attention (as in\n",
    "    LSH attention).\n",
    "  \"\"\"\n",
    "    assert v is not None\n",
    "    share_qk = k is None\n",
    "    if share_qk:\n",
    "        k = q\n",
    "        if kv_info is None:\n",
    "            kv_info = q_info\n",
    "\n",
    "    if share_qk:\n",
    "        k = length_normalized(k)\n",
    "    k = k / np.sqrt(k.shape[-1])\n",
    "\n",
    "    # Dot-product attention.\n",
    "    kr = np.swapaxes(k, -1, -2)  # note the fancy transpose for later..\n",
    "\n",
    "    ## Step 1  ##\n",
    "    dots = None\n",
    "    if verbose:\n",
    "        print(\"Our attend dots\", dots.shape)\n",
    "\n",
    "    # Masking\n",
    "    if mask_fn is not None:\n",
    "        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n",
    "\n",
    "    # Softmax.\n",
    "    # dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n",
    "    # dots = np.exp(dots - dots_logsumexp)  #original\n",
    "    ## Step 2  ##\n",
    "    # replace with our_softmax()\n",
    "    dots, dots_logsumexp = None\n",
    "    if verbose:\n",
    "        print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        assert rng is not None\n",
    "        # Dropout is broadcast across the bin dimension\n",
    "        dropout_shape = (dots.shape[-2], dots.shape[-1])\n",
    "        keep_prob = tie_in(dots, 1.0 - dropout)\n",
    "        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n",
    "        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n",
    "        dots = dots * multiplier\n",
    "\n",
    "    ## Step 3  ##\n",
    "    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n",
    "    out = None\n",
    "    if verbose:\n",
    "        print(\"Our attend out1\", out.shape)\n",
    "    out = np.reshape(out, (-1, out.shape[-1]))\n",
    "    if verbose:\n",
    "        print(\"Our attend out2\", out.shape)\n",
    "    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n",
    "    return out, dots_logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "g1861d--trPg",
    "outputId": "58a8974e-e3c8-4ec7-92a0-530df96d6d71"
   },
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "emb_len = 5\n",
    "d_qk = 3\n",
    "d_v = 4\n",
    "with fastmath.use_backend(\"jax\"):  # specify the backend for consistency\n",
    "    rng_attend = fastmath.random.get_prng(1)\n",
    "    q = k = jax.random.uniform(rng_attend, (seq_len, d_qk), dtype=np.float32)\n",
 