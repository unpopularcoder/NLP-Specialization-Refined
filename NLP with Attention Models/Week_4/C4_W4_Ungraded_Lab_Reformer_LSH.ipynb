{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVHY_ZYxWDph"
   },
   "source": [
    "# Reformer Efficient Attention: Ungraded Lab\n",
    "The videos describe two 'reforms' made to the Transformer to make it more memory and compute efficient. The *Reversible Layers* reduce memory and *Locality Sensitive Hashing(LSH)* reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\n",
    "\n",
    "Specifically, the notebook has 3 goals\n",
    "* review dot-product self attention for reference\n",
    "* examine LSH based self attention\n",
    "* extend our understanding and familiarity with Trax infrastructure\n",
    "\n",
    "## Outline\n",
    "- [Part 1:  Trax Efficient Attention classes](#1)\n",
    "- [Part 2:  Full Dot Product Self Attention](#2)\n",
    "    - [2.1  Description](#2.1)\n",
    "        - [2.1.1  our_softmax](#2.1.1)\n",
    "    - [2.2  our simple attend](#2.2)\n",
    "    - [2.3  Class OurSelfAttention](#2.3)\n",
    "- [Part 3:  Trax LSHSelfAttention](#3)\n",
    "    - [3.1  Description](#3.1)\n",
    "    - [3.2  our_hash_vectors](#3.2)\n",
    "    - [3.3  Sorting Buckets](#3.3)\n",
    "    - [3.4  Chunked dot product attention](#3.4)\n",
    "    - [3.5  OurLSHSelfAttention](#3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nhevIw88Vm2"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## Part 1.0  Trax Efficient Attention classes\n",
    "Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses 'layers' as a useful level of abstraction. Layers are often represented as *classes*. We're going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the 'forward' functions and utilize the existing attention layers as parent classes. The original code can be found at [github:trax/layers/Research/Efficient_attention](https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py). This link references release 1.3.4 but note that this is under the 'research' directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:\n",
    "<img src = \"C4W4_LN2_image11.PNG\" height=\"250\" width=\"250\">\n",
    "<center><b>Figure 1: Reference Tag 1.3.4 on github</b></center>\n",
    "\n",
    "\n",
    "\n",
    "While Trax uses classes liberally, we have not built many classes in the course so far. Let's spend a few moments reviewing the classes we will be using.\n",
    "<img src = \"C4W4_LN2_image1.PNG\" height=\"788\" width=\"1561\">\n",
    "\n",
    "<center><b>Figure 2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing.</b></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the *Forward* routine. It supports a `use_reference_code` capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each *'example, head'* independently. This simplifies our work as we need only worry about matrix operations on one *'example, head'* at a time. This loop calls *forward_unbatched*, which is the child process that we will be overriding.\n",
    "\n",
    "On the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a 'traditional' implementation of the dot product attention. We will be implementing the *forward_unbatched* version of this to highlight the differences between this and the LSH implementation.\n",
    "\n",
    "Below that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the *forward_unbatched* section of this and some of the utility functions it uses to explore its implementation in more detail.\n",
    "\n",
    "The code we will be working with is from the Trax source, and as such has impl