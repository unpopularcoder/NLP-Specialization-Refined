{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this ungraded lab, we will implement a popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper [\"BLEU: a Method for Automatic Evaluation of Machine Translation\"](https://www.aclweb.org/anthology/P02-1040.pdf), the BLEU score works by comparing \"candidate\" text to one or more \"reference\" translations. The result is better the closer the score is to 1. Let's see how to get this value in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:  BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Importing the Libraries\n",
    "\n",
    "We will first start by importing the Python libraries we will use in the first part of this lab. For learning, we will implement our own version of the BLEU Score using Numpy. To verify that our implementation is correct, we will compare our results with those generated by the [SacreBLEU library](https://github.com/mjpost/sacrebleu). This package provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. It also knows all the standard test sets and handles downloading, processing, and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (1.4.12)\n",
      "Requirement already satisfied: mecab-python3==0.996.5 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.996.5)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                  # import numpy to make numerical computations.\n",
    "import nltk                         # import NLTK to handle simple NL tasks like tokenization.\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from collections import Counter     # import the Counter module.\n",
    "!pip3 install 'sacrebleu'           # install the sacrebleu package.\n",
    "import sacrebleu                    # import sacrebleu in order compute the BLEU score.\n",
    "import matplotlib.pyplot as plt     # import pyplot in order to make some illustrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Defining the BLEU Score\n",
    "\n",
    "You have seen the formula for calculating the BLEU score in this week's lectures. More formally, we can express the BLEU score as:\n",
    "\n",
    "$$BLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1/4)}$$\n",
    "\n",
    "with the Brevity Penalty and precision defined as:\n",
    "\n",
    "$$BP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)$$\n",
    "\n",
    "$$precision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $m^{i}_{cand}$, is the count of i-gram in candidate matching the reference translation.\n",
    "* $m^{i}_{ref}$, is the count of i-gram in the reference translation.\n",
    "* $w^{i}_{t}$, is the total number of i-grams in candidate translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explaining the BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brevity Penalty (example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3gU5fr/8fedSmihBenSQVpAFhAQxI4NRECwg5WDgOixH79H/R2Px14ARRCVIoL9HLCBBRQpQuggLQJCbBSV3rl/f8yga9gkm5DJ7Gbv13XtlS3PzHx2srP3TntGVBVjjDGxK87vAMYYY/xlhcAYY2KcFQJjjIlxVgiMMSbGWSEwxpgYl+B3gPyqVKmS1q5d2+8YxhgTVRYuXLhNVdNCvRZ1haB27dpkZGT4HcMYY6KKiHyf02u2acgYY2KcFQJjjIlxVgiMMSbGWSEwxpgYZ4XAGGNinGeFQEReFZEtIrIih9dFRIaJSKaILBORU73KYowxJmderhGMBbrm8voFQAP3djMw0sMsxhhjcuDZeQSq+pWI1M6lSXdgvDr9YM8TkXIiUlVVf/Iiz5qfd/Hhsh+9GHWxlFoyiSvb1iIlKd7vKMYYj/l5Qll1YHPQ4yz3ueMKgYjcjLPWQK1atQo0scwtuxk+I7NAw8YiVZg473uevjydVrXK+x3HGOMhPwuBhHgu5FVyVHU0MBogEAgU6Eo6F7WoykUtLirIoDFpTuY27npnGT1HzuG5vq3oll7N70jGGI/4edRQFlAz6HENwLbdRIgO9Svx8dBOnFqrPPe9u4wN2/b4HckY4xE/C8EU4Fr36KHTgB1e7R8wBVO2RCLDr2xFYkIct05cxP5DR/yOZIzxgJeHj04C5gKNRCRLRG4QkQEiMsBt8hGwHsgEXgYGepXFFFzV1BSe6pXOtz/t5D8frfI7jjHGA14eNXRFHq8rcKtX0zeF55wmJ3F9xzq8OnsD7etVpGuzqn5HMsYUIjuz2ITl3