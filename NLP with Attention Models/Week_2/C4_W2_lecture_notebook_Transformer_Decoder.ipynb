
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Decoder: Ungraded Lab Notebook\n",
    "\n",
    "In this notebook, you'll explore the transformer decoder and how to implement it with Trax. \n",
    "\n",
    "## Background\n",
    "\n",
    "In the last lecture notebook, you saw how to translate the mathematics of attention into NumPy code. Here, you'll see how multi-head causal attention fits into a GPT-2 transformer decoder, and how to build one with Trax layers. In the assignment notebook, you'll implement causal attention from scratch, but here, you'll exploit the handy-dandy `tl.CausalAttention()` layer.\n",
    "\n",
    "The schematic below illustrates the components and flow of a transformer decoder. Note that while the algorithm diagram flows from the bottom to the top, the overview and subsequent Trax layer codes are top-down.\n",
    "\n",
    "<img src=\"transformer_decoder_lnb_figs/C4_W2_L6_transformer-decoder_S01_transformer-decoder.png\" width=\"1000\"/>"
   ]